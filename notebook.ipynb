{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy import percentile,median\n",
    "import pandas as pd\n",
    "from itertools import accumulate, chain, repeat, tee\n",
    "import itertools\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score,ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error,r2_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the mapping from categorical to numerical values for each feature in a dataset.\n",
    "# Binary Encoding (Not Supervised Case)\n",
    "def get_dict(encoded,original):\n",
    "    yellow_pages={}\n",
    "    for feature in cat_features:\n",
    "        filter_col = [col for col in encoded if col.startswith(feature)]\n",
    "        bits = encoded[filter_col].values\n",
    "        names = original[feature].values\n",
    "        yellow_pages[feature] ={name:bit for name,bit in zip(names,bits)}  \n",
    "    return yellow_pages\n",
    "\n",
    "# Output the mapping from categorical to numerical values for each feature in a dataset.\n",
    "# Leave One Out Encoding(Supervised Case)\n",
    "def dictionary(original,encoded):\n",
    "    dict={}\n",
    "    categorical = original.select_dtypes(include=['object']).columns.values\n",
    "    for i in categorical:\n",
    "        dict[i]={}\n",
    "        a = list(zip(original[i].values,encoded[i].values))\n",
    "        unique_pairs=[]\n",
    "        keys=[]\n",
    "        for x in a:\n",
    "            if x not in unique_pairs:\n",
    "                unique_pairs.append(x)\n",
    "        for j in unique_pairs:\n",
    "            keys.append(j[0])\n",
    "        if len(keys) > len(set(keys)):\n",
    "            print(\" mapping is not 1 - 1 \")\n",
    "            return None\n",
    "        else:\n",
    "            l = {}\n",
    "            for j in unique_pairs:\n",
    "                l[j[0]] = j[1]\n",
    "        dict[i] = l\n",
    "    return dict\n",
    "\n",
    "# Split or just suffle, encode data and return dictionary of encoding\n",
    "def prepare_data(dataset,target,features,split=True,supervised=True):\n",
    "    dataset = dataset[dataset[target].notnull()]\n",
    "    if supervised == True:\n",
    "        if split==True:\n",
    "            X = dataset[features]\n",
    "            y = dataset[target]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "            encoder = ce.leave_one_out.LeaveOneOutEncoder()\n",
    "            encoder.fit(X_train,y_train)\n",
    "            X_train_encoded = encoder.transform(X_train)\n",
    "            X_test_encoded = encoder.transform(X_test)\n",
    "            dict_ = dictionary(pd.concat([X_train,X_test],  ignore_index=True),\\\n",
    "                               pd.concat([X_train_encoded,X_test_encoded], ignore_index=True))\n",
    "            return X_train_encoded, X_test_encoded, y_train, y_test, dict_\n",
    "        else:\n",
    "            shuffled = dataset.sample(frac=1)\n",
    "            X = shuffled[features]\n",
    "            y = shuffled[target]\n",
    "            encoder = ce.leave_one_out.LeaveOneOutEncoder()\n",
    "            encoder.fit(X,y)\n",
    "            X_encoded = encoder.transform(X)\n",
    "            dict_ = dictionary(X,X_encoded)\n",
    "            return X_encoded,y, dict_\n",
    "    else:\n",
    "        shuffled = dataset.sample(frac=1)\n",
    "        X = shuffled[features]\n",
    "        y = shuffled[target]\n",
    "        encoder = ce.binary.BinaryEncoder()\n",
    "        X_encoded = encoder.fit_transform(X)\n",
    "        if split==False:\n",
    "            return X_encoded,y, X \n",
    "        else: \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2)\n",
    "            return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Calculate 95% Confidence Intervals\n",
    "def ci(scores):\n",
    "    intervals = st.t.interval(alpha=0.95,\n",
    "                              df=len(scores)-1,\n",
    "                              loc=np.mean(scores),\n",
    "                              scale=st.sem(scores)) \n",
    "    return intervals[0],median(scores),intervals[1]\n",
    "\n",
    "# Relative Root Mean Square\n",
    "def rrmse(actual,predicted):\n",
    "    avrg = sum(actual)/len(actual)\n",
    "    numerator = mean_squared_error(actual,predicted,squared=False)\n",
    "    denominator = avrg\n",
    "    return numerator/denominator\n",
    "\n",
    "my_rrmse = make_scorer(rrmse, greater_is_better=True)\n",
    "\n",
    "# Break a list xs into n equal sublists\n",
    "def chunk(xs, n):\n",
    "    assert n > 0\n",
    "    L = len(xs)\n",
    "    s, r = divmod(L, n)\n",
    "    widths = chain(repeat(s+1, r), repeat(s, n-r))\n",
    "    offsets = accumulate(chain((0,), widths))\n",
    "    b, e = tee(offsets)\n",
    "    next(e)\n",
    "    return [xs[s] for s in map(slice, b, e)]\n",
    "\n",
    "# Get n unique perturbations of a set s\n",
    "def findsubsets(s, n): \n",
    "    return [list(i) for i in itertools.combinations(s, n)] \n",
    "\n",
    "# Strech a shorter list based on where a longer has repeating streaks\n",
    "def stretch(longer,shorter):\n",
    "    if len(longer) <= len(shorter):\n",
    "        return shorter\n",
    "    list_set = list(set(longer))\n",
    "    res = shorter.copy()\n",
    "    for i in sorted(list_set):\n",
    "        counter = longer.count(i)\n",
    "        if counter > 1:\n",
    "            index = longer.index(i)\n",
    "            for j in range(counter -1):\n",
    "                res.insert(index,res[index])\n",
    "    return res\n",
    "\n",
    "# Grid search Cross Validation parameter tuning\n",
    "def best_params(model,X,y,target_name,grid):\n",
    "    search = GridSearchCV(model,grid,cv=5,n_jobs=8,verbose=10)\n",
    "    # Fit the random search model\n",
    "    search.fit(X,y)\n",
    "    print(search.best_params_)\n",
    "    results = {}\n",
    "    results['model'] = type(model).__name__\n",
    "    results['target'] = target\n",
    "    results['params'] = search.best_params_\n",
    "  # write results to file\n",
    "    return results\n",
    "\n",
    "def performance(y_test, preds,model,target):\n",
    "    \"\"\"\n",
    "    Get predictions and true values and scatter plot them.\n",
    "    Additionally plot a linear regression line on the points\n",
    "    of the Scatter plot and get its equation y = a *x + b.\n",
    "    The closer the line to the y = x line the better the prediction.\n",
    "    Additionally break the preds and truth paired set into 4 equal\n",
    "    chunks and plot the mean and variance in order to visualize\n",
    "    any trend to the error of the prediction.\n",
    "    \"\"\"\n",
    "    path = '/home/dimitris/Documents/papers/grain_of_salt/paper/images/performances/'\n",
    "#     model_name = type(model).__name__\n",
    "    model_name = 'Random Forest'\n",
    "    rmse = mean_squared_error(preds,y_test)\n",
    "    r2 = r2_score(y_test,preds)\n",
    "    \n",
    "    #    separate truth and preds in chunks\n",
    "    y_test_sorted, preds_sorted = zip(*sorted(zip(y_test, preds)))\n",
    "    zipped = list(zip(preds_sorted,y_test_sorted))\n",
    "    lists = chunk(zipped,4)\n",
    "    \n",
    "    #    make linear regression line on points (pred,truth)\n",
    "    preds_extended = sm.add_constant(preds)\n",
    "    linear_regressor = sm.OLS(y_test,preds_extended).fit()\n",
    "    #   alpha coefficient of linear regression\n",
    "    a = round(linear_regressor.params[1],2)\n",
    "    #   intercept of linear regression\n",
    "    b = round(linear_regressor.params[0],2)\n",
    "    #   get the sign of the intercept\n",
    "    score = lambda i: (\"+\" if i > 0 else \"\") + str(i)\n",
    "    Y = a*y_test +b\n",
    "    #   calculate the limits of the graph, as the min and max of both preds and truths\n",
    "    min_ = np.min(np.array(np.min(y_test),np.min(preds)))\n",
    "    max_ = np.max(np.array(np.max(y_test),np.max(preds)))\n",
    "    #   array of x's for identity line\n",
    "    x = np.linspace(min_,max_, 1000)\n",
    "    \n",
    "    ############## PLOT ##############\n",
    "    fig = plt.figure(figsize=(7.5,7.5))\n",
    "    # H0: The slope of the X variable is zero\n",
    "    label = 'regresssion:'+'y='+str(a)+'x'+score(b) +\\\n",
    "    '\\np-value: ' + str(np.format_float_scientific(linear_regressor.pvalues[1],precision=2))\\\n",
    "    +'\\n'+'R^2: '+str(round(r2,2))\n",
    "    #     +'\\nRMSE '+str(round(rmse,2))+\\\n",
    "    plt.plot(y_test,Y,'green',label=label)\n",
    "    # scatter plot of pred and truths\n",
    "    plt.scatter(y_test, preds, alpha=0.5,label='preds')\n",
    "    plt.xlim(min_,max_)\n",
    "    plt.ylim(min_,max_)\n",
    "    plt.xlabel('true values',fontsize = 18)\n",
    "    plt.ylabel('predited',fontsize=18)\n",
    "    title1 = '%s, %s' % (model_name,target)\n",
    "    plt.title(title1,fontsize = 20)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.savefig(path+target+'.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(20,4))\n",
    "    ax=[None]*4\n",
    "    means=[]\n",
    "    stds_=[]\n",
    "    for list_,index in zip(lists,range(4)):\n",
    "        res = [pred - truth for pred, truth in list_ ]\n",
    "        mean = np.mean(res)\n",
    "        std = np.std(res)\n",
    "        means.append(mean)\n",
    "        stds_.append(std)\n",
    "        ax[index] = fig.add_subplot(1,4,index+1)\n",
    "        ax[index].hist(res,bins=50,label='mean: '+str(round(mean,6)) + '\\n' + 'st.deviation: '+str(round(std,6)))\n",
    "        ax[index].set_title('slice: '+str(index +1))\n",
    "        ax[index].legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(7.5,7.5))\n",
    "    title2 = 'predicted - true: means & st.deviation, %s' % (target)\n",
    "    plt.plot(means, '--ro',label='mean',linewidth=2)\n",
    "    plt.plot(stds_ , '--go',label='st.deviation',linewidth=2)\n",
    "    plt.title(title2,fontsize=20)\n",
    "    plt.xlabel('groups',fontsize=18)\n",
    "    plt.xticks([0,1,2,3],['1st','2nd','3rd','4th'])\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.savefig(path+target+'_means.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.read_excel('./data/PIDE3.2+mcds.xlsx',convert_float=False)\n",
    "excel.rename(columns = {\n",
    "                        'ai_paper':'alpha',\n",
    "                        'bi_paper':'beta',\n",
    "                        'All Clusters/(Gbp*Gy)_hy':'All_clusters_hy',\n",
    "                        'AllClusters/(Gbp*Gy)_ox':'All_clusters_ox',\n",
    "                        'DSB/(Gbp*Gy)_hy':'DSBs_hy',\n",
    "                        'DSB/(Gbp*Gy)_ox':'DSBs_ox'\n",
    "                       }, inplace = True)\n",
    "# \n",
    "targets = ['alpha','beta','DSBs_hy','DSBs_ox','All_clusters_hy','All_clusters_ox']\n",
    "features = ['Cells','CellClass','CellOrigin','CellCycle','DNAcontent','Ion','Charge','IrradiationConditions','LET','Energy']\n",
    "cat_features = ['Cells','CellCycle','Ion','CellClass','CellOrigin','IrradiationConditions']\n",
    "excel.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,anneal, rand, mix,partial\n",
    "import json\n",
    "import os\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "rf_grid = {\n",
    "    'max_depth': range(1,40),\n",
    "    'n_estimators': range(2000),\n",
    "    'min_samples_split': [2,3,4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1,2,3],\n",
    "    'oob_score':[True,False],\n",
    "    'max_samples':[0.8,0.9],\n",
    "    'criterion':['mae','mse'],\n",
    "    }\n",
    "\n",
    "rf_space = {\n",
    "#     'max_depth': hp.choice('max_depth', range(1,40)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(10,2000,20)),\n",
    "    'min_samples_split':hp.choice('min_samples_split', [2,3,4]),\n",
    "    'max_features': hp.choice('max_features',['auto', 'sqrt', 'log2']),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf',[1,2,3]),\n",
    "    'oob_score':hp.choice('oob_score',[True,False]),\n",
    "    'max_samples':hp.choice('max_samples',[0.8,0.9]),\n",
    "    'criterion':hp.choice('criterion',['mae','mse'])\n",
    "}\n",
    "\n",
    "def hyperparameter_tuning(space):\n",
    "    model=RandomForestRegressor(n_jobs=-1,\n",
    "#                                 max_depth = int(space['max_depth']),\n",
    "                                n_estimators =space['n_estimators'],\n",
    "                                max_features = space['max_features'],\n",
    "                                min_samples_split = int(space['min_samples_split']),\n",
    "                                min_samples_leaf = int(space['min_samples_leaf']),\n",
    "                                criterion = space['criterion'],\n",
    "                                oob_score = space['oob_score'],\n",
    "                                max_samples = space['max_samples'],\n",
    "                               )\n",
    "\n",
    "    model.fit(X,y)\n",
    "    error = cross_val_score(model, X, y,n_jobs=-1,cv=5,scoring=my_rrmse).mean()\n",
    "    print (\"SCORE:\", error)\n",
    "    return {'loss':error, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "best_parameters = []\n",
    "for target in targets:\n",
    "    X, y, original = prepare_data(excel,target,features,split=False,supervised=False)\n",
    "    trials = Trials()\n",
    "    mix_algo = partial(mix.suggest,\n",
    "                       p_suggest=[(0.05, rand.suggest),\n",
    "                                  (0.75, tpe.suggest),\n",
    "                                  (0.20, anneal.suggest)])\n",
    "    best = fmin(fn=hyperparameter_tuning,\n",
    "                space=rf_space,\n",
    "                algo=mix_algo,\n",
    "                max_evals=150,\n",
    "                trials=trials)\n",
    "    print(best)\n",
    "    for key in rf_grid:\n",
    "        if type(rf_grid[key]) is list:\n",
    "            try:\n",
    "                best[key] = rf_grid[key][best[key]]\n",
    "            except:\n",
    "                print('MISSILES LAUNCHED!!!')\n",
    "                print(rf_grid[key],best[key])\n",
    "    best['n_jobs']=-1\n",
    "    print(best)\n",
    "    results = {}\n",
    "    results['model'] = 'Random Forest'\n",
    "    results['target'] = target\n",
    "    results['params'] = best\n",
    "    best_parameters.append(results)\n",
    "\n",
    "with open('best_params.txt', 'w') as param_file:\n",
    "    for element in best_parameters:\n",
    "        param_file.write(json.dumps(element,cls=NpEncoder))\n",
    "        param_file.write(os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = []\n",
    "with open('best_params.txt') as f:\n",
    "    for jsonObj in f:\n",
    "        paramDict = json.loads(jsonObj)\n",
    "        parameters.append(paramDict)\n",
    "\n",
    "for obj in parameters:\n",
    "    target = obj['target']\n",
    "    X_train, X_test, y_train, y_test = prepare_data(excel,target,features,split=True,supervised=False)\n",
    "    model = RandomForestRegressor()\n",
    "    params = obj['params']\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train,y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    test = np.asarray(y_test)\n",
    "    performance(test,preds, model, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootsrtap Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import explained_variance_score,r2_score\n",
    "import scipy.stats as st\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "for obj in parameters:\n",
    "    target = obj['target']\n",
    "    X,y,original = prepare_data(excel,target,features,split=False,supervised=False)\n",
    "    print('target: ',target,'\\tsamples: ',len(X))\n",
    "    # configure bootstrap\n",
    "    n_iterations = 10\n",
    "    values = X.join(y).values\n",
    "    n_size = int(len(values) * 0.5)\n",
    "    # run bootstrap\n",
    "    stats = []\n",
    "    for i in range(n_iterations):\n",
    "        # prepare train and test sets\n",
    "        train = resample(values, n_samples=n_size)\n",
    "        test = np.array([x for x in values if x.tolist() not in train.tolist()])\n",
    "        # name them more sensibly\n",
    "        X_train = train[:,:-1]\n",
    "        y_train = train[:,-1]\n",
    "        X_test = test[:,:-1]\n",
    "        y_test = test[:,-1]\n",
    "\n",
    "        # fit model\n",
    "        model = RandomForestRegressor()\n",
    "        params = obj['params']\n",
    "        model.set_params(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # evaluate model\n",
    "        predictions = model.predict(X_test)\n",
    "        score = rrmse(y_test, predictions)\n",
    "        print(score)\n",
    "        stats.append(score)\n",
    "    # confidence intervals\n",
    "    print(ci(stats),'\\t', target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_rel\n",
    "def statistical_model(X,y,target,features):\n",
    "    dataset = pd.concat([X,y],axis=1)\n",
    "    formula = target + ' ~ '+' + '.join(features)\n",
    "    model = smf.glm(formula = formula,data=dataset, family=sm.families.Poisson()).fit()\n",
    "    model = smf.ols(formula ,data=dataset).fit()\n",
    "    return model\n",
    "    \n",
    "families =[\n",
    "    sm.families.Gamma(),\n",
    "    sm.families.Binomial(),\n",
    "    sm.families.Gaussian(),\n",
    "    sm.families.Poisson(),\n",
    "    \n",
    "]\n",
    "for obj in parameters:\n",
    "    target = obj['target']\n",
    "    print(target)\n",
    "    stat_mse=[]\n",
    "    rf_mse = []\n",
    "    for i in range(10):\n",
    "        dataset = excel[excel[target].notnull()]\n",
    "        X = dataset[features]\n",
    "        y = dataset[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "        encoder = ce.leave_one_out.LeaveOneOutEncoder()\n",
    "        encoder.fit(X_train,y_train)\n",
    "        X_train_encoded = encoder.transform(X_train)\n",
    "        X_test_encoded = encoder.transform(X_test)\n",
    "        \n",
    "        model = RandomForestRegressor()\n",
    "        params = obj['params']\n",
    "        model.set_params(**params)\n",
    "        model.fit(X_train_encoded,y_train)\n",
    "        rf_preds = model.predict(X_test_encoded)\n",
    "        rf_mse.append(rrmse(y_test,rf_preds))\n",
    "\n",
    "        stat_model = statistical_model(X_train_encoded,y_train,target,features)\n",
    "        stat_preds = stat_model.predict(X_test_encoded)\n",
    "        stat_mse.append(rrmse(y_test,stat_preds))\n",
    "        \n",
    "    stat, p = ttest_rel(stat_mse, rf_mse)\n",
    "    print('\\tMean rf_mse=%.3f,\\n\\tMean stat_mse=%.3f,\\n\\tStatistics=%.3f,\\n\\tp=%s' \\\n",
    "          % (sum(rf_mse)/len(rf_mse),sum(stat_mse)/len(stat_mse),stat, np.format_float_scientific(p,precision=3)))\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('\\tSame distributions (fail to reject H0)')\n",
    "    else:\n",
    "        print('\\tDifferent distributions (reject H0)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "import sys\n",
    "\n",
    "path = '/home/dimitris/Documents/papers/grain_of_salt/paper/images/'\n",
    "\n",
    "def plot_importance(model,features, target,X):\n",
    "    file = '%simportances/%s.png' % (path,target)\n",
    "    title = 'Feature Importance, %s' % (target)\n",
    "    ########## Importance ###############\n",
    "    short_features = ['Ir_cond' if a == 'IrradiationConditions' else a for a in features]\n",
    "    unpacked_importance = model.feature_importances_\n",
    "    aggregate_importnance = []\n",
    "    for feature in features:\n",
    "        filter_col = [col for col in X if col.startswith(feature)]\n",
    "        indeces = [X.columns.get_loc(col) for col in filter_col]\n",
    "        aggregate_importnance.append(sum([unpacked_importance[index] for index in indeces]))\n",
    "    x = aggregate_importnance\n",
    "    max_ = max(x)*1.1\n",
    "    y = short_features\n",
    "    sorted_list = sorted(list(zip(y,x)),key=lambda tup: tup[1])\n",
    "    df = pd.DataFrame(sorted_list, columns = ['Features', 'Importance'])\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    splot=sns.barplot(x='Features',y='Importance',data=df)\n",
    "    \n",
    "    for p,name in zip(splot.patches,df['Features'].values):\n",
    "        offset = len(name)*6\n",
    "        if len(name)/40+p.get_height() > max_ :\n",
    "            print(name)\n",
    "            offset = -len(name)*5\n",
    "        splot.annotate(name, \n",
    "        (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "        ha = 'center', va = 'center', \n",
    "        size=16,\n",
    "        xytext = (0, offset), \n",
    "        textcoords = 'offset points',\n",
    "        rotation=90)\n",
    "    plt.tick_params(labelbottom=False,\n",
    "                    axis='x',\n",
    "                    which='both',\n",
    "                    bottom=False,\n",
    "                    top=False)\n",
    "    \n",
    "    plt.xlabel(\"Features\", size=16)\n",
    "    plt.ylabel(\"Importance\", size=16)\n",
    "    plt.title(title,fontsize=18)\n",
    "    plt.savefig(file)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "# Plot partial dependence for categorical features\n",
    "def categorical_pd(model,X,feature,dict_):\n",
    "    if feature != 'Cells':\n",
    "        return None\n",
    "    file = '%spd/%s_%s.png' % (path,target,feature)\n",
    "    filter_col = [col for col in X if col.startswith(feature)]\n",
    "    yellow_pages = dict_[feature]\n",
    "    pd = []\n",
    "    for name in yellow_pages:\n",
    "        X_copy = X.copy()\n",
    "        for col,value in zip(filter_col,yellow_pages[name]):\n",
    "            X_copy[col].values[:] = value\n",
    "        preds = model.predict(X_copy)\n",
    "        pd.append(np.mean(preds))\n",
    "    x,y = yellow_pages.keys() ,pd\n",
    "    title= 'Partial Dependence, %s' % (feature)\n",
    "    if feature in ['Cells','Ion']:\n",
    "        size = (16, 7.5)\n",
    "    else:\n",
    "        size = (7.5,7.5)\n",
    "    rotation = (0,90)[feature =='Cells']\n",
    "    zipped = list(zip(x,y))\n",
    "    sorted_zipped = sorted(zipped, key=lambda tup: tup[1])\n",
    "    locs = list(range(len(x)))\n",
    "    x,y=zip(*sorted_zipped)\n",
    "    width = (0.8,0.2)[len(locs) == 2]\n",
    "    plt.figure(figsize=size)\n",
    "    plt.title(title, fontsize =20)        \n",
    "    plt.xticks(locs,x,rotation=rotation,fontsize=12)\n",
    "    plt.xlabel(feature,fontsize=16)\n",
    "    plt.ylabel('Predicted Outcome',fontsize=16)\n",
    "    plt.bar(x,y)\n",
    "    plt.ylim([min(y)*0.9,max(y)*1.05])\n",
    "    plt.savefig(file)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "# Plot partial dependence for numerical features\n",
    "def numerical_pd(feature,model,X):\n",
    "    file = '%spd/%s_%s.png' % (path,target,feature)\n",
    "    y,x = partial_dependence(model, X, [feature])\n",
    "    x_,y_ = x[0].tolist(),y[0].tolist()\n",
    "    title= 'Partial Dependence, %s' % (feature)\n",
    "    ymax = max(y_)\n",
    "    xpos = y_.index(ymax)\n",
    "    xmax = x_[xpos]\n",
    "    label = feature + '= ' + str(round(xmax,2))\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(title, fontsize =20)\n",
    "    plt.xlabel(feature,fontsize=16)\n",
    "    plt.ylabel('Predicted Outcome',fontsize=16)\n",
    "    plt.plot(x_,y_,'tab:red',linewidth=2)\n",
    "    plt.annotate(label, xy=(xmax, ymax), fontsize=16)\n",
    "    plt.savefig(file)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "def two_way(est,X_train,target):\n",
    "    pair = ('Energy', 'LET')\n",
    "    file = '%spd/%s_%s_%s.png' % (path,target,pair[0],pair[1])\n",
    "    title = target+'\\nEnergy - LET Partial Dependence'\n",
    "    fig = plt.figure()\n",
    "    pdp, axes = partial_dependence(est, X_train, features=pair,\n",
    "                                   grid_resolution=20)\n",
    "    XX, YY = np.meshgrid(axes[0], axes[1])\n",
    "    Z = pdp[0].T\n",
    "    ax = Axes3D(fig)\n",
    "    surf = ax.plot_surface(XX, YY, Z, \n",
    "                           cmap=plt.cm.viridis\n",
    "                           , edgecolor='k')\n",
    "    ax.set_xlabel(pair[0],fontsize=14)\n",
    "    ax.set_ylabel(pair[1],fontsize=14,rotation=15)\n",
    "    ax.set_zlabel('Partial dependence')\n",
    "    #  pretty init view\n",
    "    ax.view_init(elev=22, azim=30)\n",
    "    plt.colorbar(surf)\n",
    "    plt.yticks(rotation=15)\n",
    "    plt.suptitle(title,fontsize=18)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(file)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return None\n",
    "        \n",
    "def local_interpretation(model,X,instance, target, original_instance):\n",
    "    file = '%slime/%s.png' % (path,target)\n",
    "    title = 'Local interpretation: %s' % (target)\n",
    "    predict = lambda z: model.predict(z).astype(float)\n",
    "    prediction = predict(instance.reshape(1,-1))\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(X.values,\n",
    "                                                       feature_names=X.columns.values,\n",
    "                                                       verbose=False,\n",
    "                                                       mode='regression')\n",
    "    \n",
    "    explanation = explainer.explain_instance(instance,\n",
    "                                             predict,\n",
    "                                             num_features=28,\n",
    "                                             num_samples=100,\n",
    "                                             model_regressor=LinearRegression(fit_intercept=False)   \n",
    "                                            )\n",
    "\n",
    "    unpack = lambda pair:(pair[0].split()[0],float(pair[0].split()[2])) if (len(pair[0].split()) == 3)\\\n",
    "             else (pair[0].split()[2], float(pair[0].split()[4]))\n",
    "                            \n",
    "    lime_list = explanation.as_list()\n",
    "    results = []\n",
    "    for feature in cat_features:\n",
    "        impact = 0\n",
    "        filter_col = [col for col in X if col.startswith(feature)]\n",
    "        for item in lime_list:\n",
    "            unpacked_item = unpack(item)\n",
    "            if unpacked_item[0] in filter_col:\n",
    "                impact += item[1]\n",
    "        name = original_instance[feature]\n",
    "        result = '%s=%s'% (feature,name)\n",
    "        results.append((result,impact))\n",
    "    encoded_features = [col for col in X for feature in cat_features if col.startswith(feature)]\n",
    "    unpacked_list = list(map(unpack,lime_list))\n",
    "    for lime_item, unpacked_item in zip(lime_list,unpacked_list):\n",
    "            if unpacked_item[0] not in encoded_features:\n",
    "                results.append(lime_item)\n",
    "    results = sorted(results ,key = lambda tup:abs(tup[1]))\n",
    "\n",
    "    impacts = [i[1] for i in results]\n",
    "    y_labels = [i[0] for i in results]\n",
    "    impact_series = pd.Series(impacts)\n",
    "    # Plot the figure.\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.barh(list(range(len(impacts))),impacts)\n",
    "    ax.set_title('Local Interpretation, '+target, fontsize=20)\n",
    "    ax.set_ylabel('Impacts', fontsize=16)\n",
    "    ax.set_xlabel('Predicted Outcome', fontsize=16)\n",
    "    max_value = max(impacts)\n",
    "    min_value = min(impacts)\n",
    "    unit = (abs(1.4*min_value)+max_value*1.1)/500\n",
    "    ax.set_xlim([1.4*min_value, max_value*1.1])\n",
    "    ax.set_yticklabels([])\n",
    "    rects = ax.patches\n",
    "    # Annotations\n",
    "    for rect,impact,y_label in zip(rects,impacts,y_labels):\n",
    "        rect.set_height(0.66)\n",
    "        y_value = rect.get_y() + rect.get_height()/2\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = 2 if impact > 0 else -2\n",
    "        color = 'g' if impact >0 else 'r'\n",
    "        rect.set_color(color)\n",
    "        ha = 'left' if impact > 0 else 'right'\n",
    "        offset = unit*2 if impact > 0 else -unit*2\n",
    "        label = \"{:.1f}\".format(impact)\n",
    "        ax.text(impact+offset,y_value,label,size=12,transform=ax.transData,ha=ha,va='center')\n",
    "        ax.text(-offset,y_value,\n",
    "                y_label,size=12,\n",
    "                transform=ax.transData,\n",
    "                ha='left' if ha=='right' else 'right',\n",
    "                va='center')\n",
    "    # prediction\n",
    "    ax.text(0.985, 0.015, 'prediction: '+str(round(prediction[0],2)),size=12,\n",
    "            ha=\"right\", va=\"bottom\",\n",
    "            transform=ax.transAxes, \n",
    "            bbox=dict(boxstyle=\"square\",\n",
    "#             ec='tab:gray',\n",
    "            fc='white',\n",
    "            ))\n",
    "    plt.savefig(file)                   \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return result\n",
    "\n",
    "parameters = []\n",
    "with open('best_params.txt') as f:\n",
    "    for jsonObj in f:\n",
    "        paramDict = json.loads(jsonObj)\n",
    "        parameters.append(paramDict)\n",
    "        \n",
    "for obj in parameters:\n",
    "    target = obj['target']\n",
    "    X , y, original = prepare_data(excel,target,features,split=False,supervised=False)\n",
    "    print(target)\n",
    "    dict_ = get_dict(X,original)\n",
    "    model = RandomForestRegressor()\n",
    "    params = obj['params']\n",
    "    model.set_params(**params)\n",
    "    model.fit(X,y)\n",
    "    for feature in features:\n",
    "        if feature in cat_features:\n",
    "            categorical_pd(model,X,feature,dict_)\n",
    "        else:\n",
    "            numerical_pd(feature,model,X)\n",
    "    plot_importance(model,features, target,X)\n",
    "    two_way(model,X,target)\n",
    "    local_interpretation(model,X,X.iloc[10].values,target,original.iloc[10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
