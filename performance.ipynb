{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import percentile,median\n",
    "import pandas as pd\n",
    "from itertools import accumulate, chain, repeat, tee\n",
    "import itertools\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score,ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error,r2_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the mapping from categorical to numerical values for each feature in a dataset\n",
    "# given both categorical and encoded dataframes, works only if mapping is 1 - 1 \n",
    "def dictionary(original,encoded):\n",
    "    dict={}\n",
    "    categorical = original.select_dtypes(include=['object']).columns.values\n",
    "    for i in categorical:\n",
    "        dict[i]={}\n",
    "        a = list(zip(original[i].values,encoded[i].values))\n",
    "        unique_pairs=[]\n",
    "        keys=[]\n",
    "        for x in a:\n",
    "            if x not in unique_pairs:\n",
    "                unique_pairs.append(x)\n",
    "        for j in unique_pairs:\n",
    "            keys.append(j[0])\n",
    "        if len(keys) > len(set(keys)):\n",
    "            print(\"MISSILES LAUNCHED!!!!\")\n",
    "            return None\n",
    "        else:\n",
    "            l = {}\n",
    "            for j in unique_pairs:\n",
    "                l[j[0]] = j[1]\n",
    "        dict[i] = l\n",
    "    return dict\n",
    "\n",
    "# Split or just suffle, encode data and return dictionary of encoding\n",
    "def prepare_data(dataset,target,features,split=True):\n",
    "    dataset = dataset[dataset[target].notnull()]\n",
    "    if split==True:\n",
    "        X = dataset[features]\n",
    "        y = dataset[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        encoder = ce.leave_one_out.LeaveOneOutEncoder()\n",
    "        encoder.fit(X_train,y_train)\n",
    "        X_train_encoded = encoder.transform(X_train)\n",
    "        X_test_encoded = encoder.transform(X_test)\n",
    "        dict_ = dictionary(pd.concat([X_train,X_test],  ignore_index=True),\\\n",
    "                           pd.concat([X_train_encoded,X_test_encoded], ignore_index=True))\n",
    "        return X_train_encoded, X_test_encoded, y_train, y_test, dict_\n",
    "    else:\n",
    "        shuffled = dataset.sample(frac=1)\n",
    "        X = shuffled[features]\n",
    "        y = shuffled[target]\n",
    "        encoder = ce.leave_one_out.LeaveOneOutEncoder()\n",
    "        encoder.fit(X,y)\n",
    "        X_encoded = encoder.transform(X)\n",
    "        dict_ = dictionary(X,X_encoded)\n",
    "        return X_encoded,y, dict_\n",
    "\n",
    "# Calculate 95% Confidence Intervals\n",
    "def ci(scores):\n",
    "    alpha =0.5\n",
    "    lower_p = alpha/2.0\n",
    "    lower = max(0.0, percentile(scores, lower_p))\n",
    "    upper_p = (100 - alpha) + (alpha / 2.0)\n",
    "    upper = min(1.0, percentile(scores, upper_p))\n",
    "    return list(map(lambda x: round(x,4),[lower,median(scores),upper]))\n",
    "\n",
    "# Relative Root Mean Square\n",
    "def rrmse(actual,predicted):\n",
    "    avrg = sum(actual)/len(actual)\n",
    "    numerator = sum([(x - y)**2 for x,y in zip(actual,predicted)])   \n",
    "    denominator = sum([(x - avrg)**2 for x in actual])\n",
    "    return numerator/denominator\n",
    "\n",
    "my_rrmse = make_scorer(rrmse, greater_is_better=True)\n",
    "\n",
    "# Cross validation to find best parameters\n",
    "def best_params(X,y,model,grid):\n",
    "    random = GridSearchCV(model,grid,cv=5,n_jobs=8,verbose=0)\n",
    "    random.fit(X,y)\n",
    "    return random.best_params_\n",
    "\n",
    "# break a list xs into n equal sublists\n",
    "def chunk(xs, n):\n",
    "    assert n > 0\n",
    "    L = len(xs)\n",
    "    s, r = divmod(L, n)\n",
    "    widths = chain(repeat(s+1, r), repeat(s, n-r))\n",
    "    offsets = accumulate(chain((0,), widths))\n",
    "    b, e = tee(offsets)\n",
    "    next(e)\n",
    "    return [xs[s] for s in map(slice, b, e)]\n",
    "\n",
    "# get n unique perturbations of a set s\n",
    "def findsubsets(s, n): \n",
    "    return [list(i) for i in itertools.combinations(s, n)] \n",
    "\n",
    "# strech a shorter list based on where a longer has repeating streaks\n",
    "def stretch(longer,shorter):\n",
    "    if len(longer) <= len(shorter):\n",
    "        return shorter\n",
    "    list_set = list(set(longer))\n",
    "    res = shorter.copy()\n",
    "    for i in sorted(list_set):\n",
    "        counter = longer.count(i)\n",
    "        if counter > 1:\n",
    "            index = longer.index(i)\n",
    "            for j in range(counter -1):\n",
    "                res.insert(index,res[index])\n",
    "    return res\n",
    "\n",
    "# Grid search Cross Validation parameter tuning\n",
    "def best_params(model,X,y,target_name,grid):\n",
    "    search = GridSearchCV(model,grid,cv=5,n_jobs=8,verbose=10)\n",
    "    # Fit the random search model\n",
    "    search.fit(X,y)\n",
    "    print(search.best_params_)\n",
    "    results = {}\n",
    "    results['model'] = type(model).__name__\n",
    "    results['target'] = target\n",
    "    results['params'] = search.best_params_\n",
    "  # write results to file\n",
    "    return results\n",
    "\n",
    "def plot_pd(feature,cat_features,model, X,dict_):\n",
    "    labels,locs,rotation = (None,None,0)\n",
    "    y,x = partial_dependence(model, X, [feature])\n",
    "    x_,y_ = x[0].tolist(),y[0].tolist()\n",
    "    title = 'Partial Dependence for '+feature\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(title, fontsize =20)\n",
    "    if feature in cat_features:\n",
    "        rotation = (0,90)[feature =='Cells']\n",
    "        locs = list(dict_[feature].values())\n",
    "        labels = list(dict_[feature].keys())\n",
    "        zipped = list(zip(locs,labels))\n",
    "        sorted_zipped = sorted(zipped, key=lambda tup: tup[0])\n",
    "        locs = [pair[0] for pair in sorted_zipped]\n",
    "        labels = [pair[1] for pair in sorted_zipped]\n",
    "        x_ = list(range(len(locs)))\n",
    "        y_ = stretch(locs, y_)\n",
    "        locs = x_\n",
    "        width = (0.8,0.2)[len(locs) == 2]\n",
    "        if feature == 'Ion':\n",
    "            zipped = list(zip(x_,y_,labels))\n",
    "            sorted_zipped = sorted(zipped, key=lambda tup: int(re.search(r'\\d+', tup[2]).group()))\n",
    "            x_ = list(range(len(locs)))\n",
    "            y_ = [pair[1] for pair in sorted_zipped]\n",
    "            labels = [pair[2] for pair in sorted_zipped]\n",
    "        plt.xticks(locs,labels,rotation=rotation)\n",
    "        plt.xlabel(feature,fontsize=16)\n",
    "        plt.ylabel('Predicted Outcome',fontsize=16)\n",
    "        plt.bar(x_,y_,width)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.xlabel(feature,fontsize=16)\n",
    "        plt.ylabel('Predicted Outcome',fontsize=16)\n",
    "        plt.plot(x_,y_,'tab:red')\n",
    "        ymax = max(y_)\n",
    "        xpos = y_.index(ymax)\n",
    "        xmax = x_[xpos]\n",
    "        label = feature + '= ' + str(round(xmax,2))\n",
    "        plt.annotate(label, xy=(xmax, ymax))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return None\n",
    "\n",
    "def performance(y_test, preds,model,target):\n",
    "    \"\"\"\n",
    "    Get predictions and true values and scatter plot them.\n",
    "    Additionally plot a linear regression line on the points\n",
    "    of the Scatter plot and get its equation y = a *x + b.\n",
    "    The closer the line to the y = x line the better the prediction.\n",
    "    Additionally break the preds and truth paired set into 4 equal\n",
    "    chunks and plot the mean and variance in order to visualize\n",
    "    any trend to the error of the prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = type(model).__name__\n",
    "    rmse = mean_squared_error(preds,y_test)\n",
    "    r2 = r2_score(y_test,preds)\n",
    "    \n",
    "    #    separate truth and preds in chunks\n",
    "    y_test_sorted, preds_sorted = zip(*sorted(zip(y_test, preds)))\n",
    "    zipped = list(zip(preds_sorted,y_test_sorted))\n",
    "    lists = chunk(zipped,4)\n",
    "    \n",
    "    #    make linear regression line on points (pred,truth)\n",
    "    preds_extended = sm.add_constant(preds)\n",
    "    linear_regressor = sm.OLS(y_test,preds_extended).fit()\n",
    "    #   alpha coefficient of linear regression\n",
    "    a = round(linear_regressor.params[1],5)\n",
    "    #   intercept of linear regression\n",
    "    b = round(linear_regressor.params[0],5)\n",
    "    #   get the sign of the intercept\n",
    "    score = lambda i: (\"+\" if i > 0 else \"\") + str(i)\n",
    "    Y = a*y_test +b\n",
    "    #   calculate the limits of the graph, as the min and max of both preds and truths\n",
    "    min_ = np.min(np.array(np.min(y_test),np.min(preds)))\n",
    "    max_ = np.max(np.array(np.max(y_test),np.max(preds)))\n",
    "    #   array of x's for identity line\n",
    "    x = np.linspace(min_,max_, 1000)\n",
    "    \n",
    "    ############## PLOT ##############\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    gs = gridspec.GridSpec(nrows=3, ncols=4,height_ratios=[2, 2,1])\n",
    "    # H0: The beta coefficient(slope) of the X variable is zero\n",
    "    label = 'regresssion:'+'y='+str(a)+'x'+score(b) +\\\n",
    "    '\\nPvalue: ' + str(np.format_float_scientific(linear_regressor.pvalues[1],precision=4))\\\n",
    "    +'\\nRMSE '+str(round(rmse,6))+\\\n",
    "    '\\n'+'R^2: '+str(round(r2,6))\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.plot(y_test,Y,'green',label=label)\n",
    "    # scatter plot of pred and truths\n",
    "    ax1.scatter(y_test, preds, alpha=0.5,label='preds')\n",
    "    ax1.set_xlim(min_,max_)\n",
    "    ax1.set_ylim(min_,max_)\n",
    "    ax1.set_xlabel('true values',fontsize = 16)\n",
    "    ax1.set_ylabel('predited',fontsize=16)\n",
    "    ax1.set_title(model_name+' '+target,fontsize = 16)\n",
    "    ax1.legend(fontsize=16)\n",
    "   \n",
    "    ax=[None]*4\n",
    "    means=[]\n",
    "    vars_=[]\n",
    "    for list_,index in zip(lists,range(4)):\n",
    "        res = [pred - truth for pred, truth in list_ ]\n",
    "        mean = np.mean(res)\n",
    "        var = np.var(res)\n",
    "        means.append(mean)\n",
    "        vars_.append(var)\n",
    "        ax[index] = fig.add_subplot(gs[2, index])\n",
    "        ax[index].hist(res,bins=50,label='mean: '+str(round(mean,6)) + '\\n' + 'variation: '+str(round(var,6)))\n",
    "        ax[index].set_title('slice: '+str(index +1))\n",
    "        ax[index].legend()\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    ax2.plot(means, color='r',  marker='.',label='mean')\n",
    "    ax2.plot(vars_ , color='g',  marker='.',label='variance')\n",
    "    ax2.set_title('predicted - true means & variance',fontsize=16)\n",
    "    ax2.set_xticks([])\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.read_excel('./data/PIDE3.2+mcds.xlsx',convert_float=False)\n",
    "excel.rename(columns = {'All Clusters/(Gbp*Gy)_hy':'DSBs_hy', 'AllClusters/(Gbp*Gy)_ox':'DSBs_ox'}, inplace = True) \n",
    "targets = ['ai_paper','bi_paper','DSBs_hy','DSBs_ox']\n",
    "features = ['Cells','CellClass','CellOrigin','CellCycle','DNAcontent','Ion','Charge','IrradiationConditions','LET','Energy']\n",
    "cat_features = ['Cells','CellCycle','Ion','CellClass','CellOrigin','IrradiationConditions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "rf_grid = {\n",
    "        'n_estimators': [10,50,100,500,1000],\n",
    "        'max_features': ['log2', 'sqrt',None],\n",
    "        'max_depth': [None],\n",
    "        'min_samples_split': [1,2,3],\n",
    "        'min_samples_leaf': [1, 2, 4,8],\n",
    "        'bootstrap': [True,False],\n",
    "        'criterion':['mae','mse'],\n",
    "        'oob_score':[True,False],\n",
    "        'max_samples':[0.9,1.0]\n",
    "}\n",
    "\n",
    "best_parameters=[]\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    X_train, X_test, y_train, y_test, encoding_dict_ = prepare_data(excel,target,features)\n",
    "    model = RandomForestRegressor()\n",
    "    best_parameters.append(best_params(model,X_train,y_train,target,rf_grid))\n",
    "\n",
    "with open('best_params.txt', 'w') as param_file:\n",
    "    for element in best_parameters:\n",
    "        param_file.write(json.dumps(element))\n",
    "        param_file.write(os.linesep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = []\n",
    "with open('best_params.txt') as f:\n",
    "    for jsonObj in f:\n",
    "        paramDict = json.loads(jsonObj)\n",
    "        parameters.append(paramDict)\n",
    "# print(parameters)\n",
    "\n",
    "for obj in parameters:\n",
    "    target = obj['target']\n",
    "    X_train, X_test, y_train, y_test, encoding_dict_ = prepare_data(excel,target,features)\n",
    "    model = RandomForestRegressor()\n",
    "    params = obj['params']\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train,y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    test = np.asarray(y_test)\n",
    "    performance(test,preds, model, target)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "for obj in parameters:\n",
    "    target = obj['target']\n",
    "    columns = [i for i in features]\n",
    "    columns.append(target)\n",
    "    data = excel[columns][excel[target].notnull()]\n",
    "    values = data.values\n",
    "    # configure bootstrap\n",
    "    n_iterations = 100\n",
    "    n_size = int(len(data) * 0.9)\n",
    "    # run bootstrap\n",
    "    stats = []\n",
    "    for i in range(n_iterations):\n",
    "\n",
    "        # prepare train and test sets\n",
    "        train = resample(values, n_samples=n_size)\n",
    "        test = np.array([x for x in values if x.tolist() not in train.tolist()])\n",
    "\n",
    "        # name them more sensibly\n",
    "        X_train = train[:,:-1]\n",
    "        y_train = train[:,-1]\n",
    "        X_test = test[:,:-1]\n",
    "        y_test = test[:,-1]\n",
    "\n",
    "        # encode them\n",
    "        encoder = ce.leave_one_out.LeaveOneOutEncoder()\n",
    "        encoder.fit(X_train,y_train)\n",
    "        X_train_encoded = encoder.transform(X_train)\n",
    "        X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "        # fit model\n",
    "        model = RandomForestRegressor()\n",
    "        params = obj['params']\n",
    "        model.set_params(**params)\n",
    "        model.fit(X_train_encoded, y_train)\n",
    "\n",
    "        # evaluate model\n",
    "        predictions = model.predict(X_test_encoded)\n",
    "        score = rrmse(y_test, predictions)\n",
    "        stats.append(score)\n",
    "    # confidence intervals\n",
    "    print(ci(stats),'\\t', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_rel\n",
    "def statistical_model(X,y,target,features):\n",
    "    dataset = pd.concat([X,y],axis=1)\n",
    "    formula = target + ' ~ '+' + '.join(features)\n",
    "    model = smf.glm(formula = formula,data=dataset, family=sm.families.Poisson()).fit()\n",
    "#     model = smf.ols(formula ,data=dataset).fit()\n",
    "    return model\n",
    "    \n",
    "families =[\n",
    "    sm.families.Gamma(),\n",
    "    sm.families.Binomial(),\n",
    "    sm.families.Gaussian(),\n",
    "    sm.families.Poisson(),\n",
    "    \n",
    "]\n",
    "for obj in parameters:\n",
    "    target = obj['target']\n",
    "    print(target)\n",
    "    stat_mse=[]\n",
    "    rf_mse = []\n",
    "    for i in range(10):\n",
    "        dataset = excel[excel[target].notnull()]\n",
    "        X = dataset[features]\n",
    "        y = dataset[target]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "        encoder = ce.leave_one_out.LeaveOneOutEncoder()\n",
    "        encoder.fit(X_train,y_train)\n",
    "        X_train_encoded = encoder.transform(X_train)\n",
    "        X_test_encoded = encoder.transform(X_test)\n",
    "        \n",
    "        model = RandomForestRegressor()\n",
    "        params = obj['params']\n",
    "        model.set_params(**params)\n",
    "        model.fit(X_train_encoded,y_train)\n",
    "        rf_preds = model.predict(X_test_encoded)\n",
    "        rf_mse.append(mean_squared_error(y_test,rf_preds))\n",
    "\n",
    "        stat_model = statistical_model(X_train_encoded,y_train,target,features)\n",
    "        stat_preds = stat_model.predict(X_test_encoded)\n",
    "        stat_mse.append(mean_squared_error(y_test,stat_preds))\n",
    "        \n",
    "    stat, p = ttest_rel(stat_mse, rf_mse)\n",
    "    print('\\tMean rf_mse=%.3f,\\n\\tMean stat_mse=%.3f,\\n\\tStatistics=%.3f,\\n\\tp=%s' \\\n",
    "          % (sum(rf_mse)/len(rf_mse),sum(stat_mse)/len(stat_mse),stat, np.format_float_scientific(p,precision=3)))\n",
    "    # interpret\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('\\tSame distributions (fail to reject H0)')\n",
    "    else:\n",
    "        print('\\tDifferent distributions (reject H0)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skater.core.explanations import Interpretation\n",
    "from skater.data import DataManager\n",
    "from skater.model import InMemoryModel\n",
    "from skater.model import DeployedModel\n",
    "from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "def interpretations(model,target,X,dict_,number):\n",
    "    ########## Importance ###############\n",
    "    print(target+\" Feature Importances\")\n",
    "    pd.Series(model.feature_importances_, index=features).nlargest(11).\\\n",
    "    plot(kind='barh',title='Feature Importane for '+target)\n",
    "    ########## Partial Dependence #######\n",
    "    print(target + ' Partial Dependene')\n",
    "    for feature in features:\n",
    "        plot_pd(feature,cat_features, model,X, dict_)\n",
    "    ########## 2-way partial dependence plots #####\n",
    "    predict = lambda z: model.predict(z).astype(float)\n",
    "    interpreter = Interpretation()\n",
    "    annotated_model = InMemoryModel(predict, examples=X)\n",
    "    interpreter = Interpretation()\n",
    "    interpreter.load_data(X,feature_names=features)\n",
    "    print(\"2-way partial dependence plots\")\n",
    "    pdp_features = [('Energy','LET')]\n",
    "    interpreter.partial_dependence.plot_partial_dependence(\n",
    "    pdp_features, annotated_model, grid_resolution=30,\n",
    "    )\n",
    "    plt.title('Energy-LET Dependence :\\n'+ target)\n",
    "    return None\n",
    "\n",
    "def local_interpretation(model,X,features,cat_features,instance, target):\n",
    "    predict = lambda z: model.predict(z).astype(float)\n",
    "    # create an explainer\n",
    "    explainer = LimeTabularExplainer(X.values, feature_names=features,\n",
    "                                     mode=\"regression\",\n",
    "                                     kernel_width=3,\n",
    "                                     categorical_features=cat_features,\n",
    "                                    )\n",
    "    # explain something\n",
    "    explanation = explainer.explain_instance(instance,predict, num_samples=10)\n",
    "    # show the explanation\n",
    "#     explanation.show_in_notebook()\n",
    "    explanation.as_pyplot_figure()\n",
    "    plt.tight_layout()\n",
    "    plt.title(target + ' Local Interpretation')\n",
    "#     plt.close()\n",
    "    return None\n",
    "\n",
    "parameters = []\n",
    "with open('best_params.txt') as f:\n",
    "    for jsonObj in f:\n",
    "        paramDict = json.loads(jsonObj)\n",
    "        parameters.append(paramDict)\n",
    "\n",
    "    for obj in parameters:\n",
    "        target = obj['target']\n",
    "        X , y , encoding_dict_ = prepare_data(excel,target,features,split=False)\n",
    "        model = RandomForestRegressor()\n",
    "        params = obj['params']\n",
    "        model.set_params(**params)\n",
    "        model.fit(X,y)\n",
    "        print(target+' global Interpretations')\n",
    "        interpretations(model,target,X, encoding_dict_,counter)\n",
    "        print(target+' local Interpretations')\n",
    "        local_interpretation(model,X,features,cat_features,X.iloc[0].values,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
